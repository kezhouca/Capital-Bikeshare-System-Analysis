{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trip Prediction with Machine Learning in Spark\n",
    "\n",
    "We are interested in seeing how weather and temperature impact daily trips. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkConf, SparkContext\n",
    "from pyspark.sql import SparkSession, functions, types\n",
    "from pyspark.sql.types import StructType\n",
    "from pyspark.sql.types import StructField\n",
    "from pyspark.sql.types import StringType\n",
    "from pyspark.sql.functions import *\n",
    "from datetime import datetime\n",
    "\n",
    "import sys\n",
    "import operator\n",
    "import re, string\n",
    "assert sys.version_info >= (3, 5)  # make sure we have Python 3.5+\n",
    "\n",
    "DATA_PATH='/user/chowkec/capitalbikeshare/data/'\n",
    "OUTPUT_PATH='/user/chowkec/capitalbikeshare/output/'\n",
    "\n",
    "spark = SparkSession.builder.appName('machineLearning').getOrCreate()\n",
    "#assert spark.version >= '2.4' # make sure we have Spark 2.4+\n",
    "spark.sparkContext.setLogLevel('WARN')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## read in tripdata and weather data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create tripdate schema types\n",
    "tripdata_schema = types.StructType([\n",
    "    types.StructField('duration', types.IntegerType()),\n",
    "    types.StructField('start_date', types.TimestampType()),\n",
    "    types.StructField('end_date', types.TimestampType()),\n",
    "    types.StructField('start_station_number', types.StringType()),\n",
    "    types.StructField('start_station', types.StringType()),\n",
    "    types.StructField('end_station_number', types.StringType()),\n",
    "    types.StructField('end_station', types.StringType()),\n",
    "    types.StructField('bike_number', types.StringType()),\n",
    "    types.StructField('member_type', types.StringType()),  \n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using spark to read in the original csv file\n",
    "trip_data = spark.read.csv(DATA_PATH+'trip', header=True, schema=tripdata_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in weather data \n",
    "weather = spark.read.csv(DATA_PATH+'weather', header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method DataFrame.printSchema of DataFrame[STATION: string, NAME: string, DATE: string, AWND: string, FMTM: string, PGTM: string, PRCP: string, SNOW: string, SNWD: string, TAVG: string, TMAX: string, TMIN: string, WDF2: string, WDF5: string, WESD: string, WSF2: string, WSF5: string, WT01: string, WT02: string, WT03: string, WT04: string, WT05: string, WT06: string, WT07: string, WT08: string, WT09: string, WT11: string, WT13: string, WT14: string, WT15: string, WT16: string, WT17: string, WT18: string, WT21: string, WT22: string]>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weather.printSchema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------------------+-------------------+--------------------+--------------------+------------------+--------------------+-----------+-----------+\n",
      "|duration|         start_date|           end_date|start_station_number|       start_station|end_station_number|         end_station|bike_number|member_type|\n",
      "+--------+-------------------+-------------------+--------------------+--------------------+------------------+--------------------+-----------+-----------+\n",
      "|    2762|2017-07-01 00:01:09|2017-07-01 00:47:11|               31289|Henry Bacon Dr & ...|             31289|Henry Bacon Dr & ...|     W21474|     Casual|\n",
      "|    2763|2017-07-01 00:01:24|2017-07-01 00:47:27|               31289|Henry Bacon Dr & ...|             31289|Henry Bacon Dr & ...|     W22042|     Casual|\n",
      "|     690|2017-07-01 00:01:45|2017-07-01 00:13:16|               31122| 16th & Irving St NW|             31299|Connecticut Ave &...|     W01182|     Member|\n",
      "|     134|2017-07-01 00:01:46|2017-07-01 00:04:00|               31201|      15th & P St NW|             31267|17th St & Massach...|     W22829|     Member|\n",
      "|     587|2017-07-01 00:02:05|2017-07-01 00:11:52|               31099|Madison & N Henry St|             31907|Franklin & S Wash...|     W22223|     Casual|\n",
      "+--------+-------------------+-------------------+--------------------+--------------------+------------------+--------------------+-----------+-----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# tripdate includes \"duration\", \"start_date\", \"end_date\", \"start_station_number\", \"start_station\", \n",
    "# \"end_station_number\", \"end_station\", \"bike_number\", and \"member_type\"\n",
    "trip_data.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----+-----------+----+----+----+----+----+-------+\n",
      "|      DATE|PRCP|SNOW_AMOUNT|TMAX|TMIN|rain|snow|both|weather|\n",
      "+----------+----+-----------+----+----+----+----+----+-------+\n",
      "|2010-01-01|0.00|        0.0|  44|  29|   0|   0|   0|  sunny|\n",
      "|2010-01-02|0.00|        0.0|  30|  20|   0|   0|   0|  sunny|\n",
      "|2010-01-03|0.00|        0.0|  28|  16|   0|   0|   0|  sunny|\n",
      "|2010-01-04|0.00|        0.0|  34|  23|   0|   0|   0|  sunny|\n",
      "|2010-01-05|0.00|        0.0|  37|  27|   0|   0|   0|  sunny|\n",
      "|2010-01-06|0.00|        0.0|  39|  29|   0|   0|   0|  sunny|\n",
      "|2010-01-07|0.00|        0.0|  39|  29|   0|   0|   0|  sunny|\n",
      "|2010-01-08|0.03|        1.0|  32|  23|   1|   1|   1|   both|\n",
      "|2010-01-09|0.00|        0.0|  36|  23|   0|   0|   0|  sunny|\n",
      "|2010-01-10|0.00|        0.0|  32|  19|   0|   0|   0|  sunny|\n",
      "|2010-01-11|0.00|        0.0|  36|  20|   0|   0|   0|  sunny|\n",
      "|2010-01-12|0.00|        0.0|  35|  29|   0|   0|   0|  sunny|\n",
      "|2010-01-13|0.00|        0.0|  44|  27|   0|   0|   0|  sunny|\n",
      "|2010-01-14|0.00|        0.0|  50|  26|   0|   0|   0|  sunny|\n",
      "|2010-01-15|0.00|        0.0|  57|  28|   0|   0|   0|  sunny|\n",
      "|2010-01-16|0.00|        0.0|  52|  31|   0|   0|   0|  sunny|\n",
      "|2010-01-17|0.68|        0.0|  45|  37|   1|   0|   0|   rain|\n",
      "|2010-01-18|0.00|        0.0|  56|  37|   0|   0|   0|  sunny|\n",
      "|2010-01-19|0.00|        0.0|  60|  33|   0|   0|   0|  sunny|\n",
      "|2010-01-20|0.03|        0.0|  44|  35|   1|   0|   0|   rain|\n",
      "+----------+----+-----------+----+----+----+----+----+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "weather.createOrReplaceTempView('weather_table')\n",
    "\n",
    "# select DATE, PRCP (precipitation), SNOW, TMAX (max temperature), TMIN (min temperature)\n",
    "weather_df = spark.sql(\"SELECT DATE, PRCP, SNOW AS SNOW_AMOUNT, TMAX, TMIN FROM weather_table\")\n",
    "weather_df.createOrReplaceTempView('weather_table')\n",
    "\n",
    "# create columns \"rain\", \"snow\", \"both\" with binary number indicators according to the numbers\n",
    "weather_df = weather_df.withColumn(\"rain\", when(weather_df[\"PRCP\"] != 0.00, 1).otherwise(0))\n",
    "weather_df = weather_df.withColumn(\"snow\", when(weather_df[\"SNOW_AMOUNT\"] != 0.0, 1).otherwise(0))\n",
    "weather_df = weather_df.withColumn(\"both\", when((weather_df[\"SNOW_AMOUNT\"] != 0.0) & (weather_df[\"PRCP\"] != 0.00), 1).otherwise(0))\n",
    "\n",
    "# create column \"weather\"\n",
    "weather_df = weather_df.withColumn(\"weather\",\n",
    "      expr(\"case when both = 1 then 'both' \" +\n",
    "                       \"when rain = '1' and snow = '0' then 'rain' \" +\n",
    "                       \"when snow = '1' and rain = '0' then 'snow' \" +\n",
    "                       \"else 'sunny' end\"))\n",
    "\n",
    "weather_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method DataFrame.printSchema of DataFrame[DATE: string, PRCP: string, SNOW_AMOUNT: string, TMAX: string, TMIN: string, rain: int, snow: int, both: int, weather: string]>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weather_df.printSchema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "tripdata.createOrReplaceTempView('tripdata_table')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+\n",
      "|      DATE|count|\n",
      "+----------+-----+\n",
      "|2017-08-11|12389|\n",
      "|2017-09-11|13401|\n",
      "|2014-11-12| 9539|\n",
      "|2016-03-01| 8588|\n",
      "|2012-04-17| 6619|\n",
      "+----------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#####################################################################################\n",
    "# Number of daily trip\n",
    "daily_df = spark.sql(\"\"\"SELECT CAST(start_date AS DATE) \"\"\" + \\\n",
    "                       \"\"\"AS DATE FROM tripdata_table\"\"\")\n",
    "\n",
    "# daily_df = daily_df.withColumn(\"trips\", daily_df.groupBy('date').count())\n",
    "\n",
    "daily_df = daily_df.groupBy('DATE').count()\n",
    "# daily_df.createOrReplaceTempView('daily_table')\n",
    "# weekday_df = spark.sql(\"\"\"SELECT start_date, end_date,\"\"\" + \\\n",
    "#                        \"\"\" WEEKDAY(only_date) As date_description FROM date_table\"\"\")\n",
    "# weekday_df.createOrReplaceTempView('weekday_table')\n",
    "# weekday_df = spark.sql(\"\"\"SELECT date_description, COUNT(*) AS number_of_trip FROM \"\"\" + \\\n",
    "#                        \"\"\"weekday_table GROUP BY date_description ORDER BY date_description\"\"\")\n",
    "daily_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----+-----------+----+----+----+----+----+-------+-----+\n",
      "|      DATE|PRCP|SNOW_AMOUNT|TMAX|TMIN|rain|snow|both|weather|count|\n",
      "+----------+----+-----------+----+----+----+----+----+-------+-----+\n",
      "|2010-09-20|0.00|        0.0|  81|  64|   0|   0|   0|  sunny|  212|\n",
      "|2010-09-21|0.00|        0.0|  80|  56|   0|   0|   0|  sunny|  324|\n",
      "|2010-09-22|0.00|        0.0|  95|  67|   0|   0|   0|  sunny|  377|\n",
      "|2010-09-23|0.00|        0.0|  93|  71|   0|   0|   0|  sunny|  373|\n",
      "|2010-09-24|0.00|        0.0|  99|  72|   0|   0|   0|  sunny|  362|\n",
      "+----------+----+-----------+----+----+----+----+----+-------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# join weather_df and daily_df \n",
    "\n",
    "joined_df = weather_df.join(daily_df, 'DATE')\n",
    "# left.join(right, \"name\")\n",
    "\n",
    "joined_df = joined_df.orderBy('DATE')\n",
    "# to_date(joined_df.DATE, \"YYYY-MM-DD\").alias(\"DATE\")\n",
    "# joined_df.printSchema\n",
    "joined_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method DataFrame.printSchema of DataFrame[DATE: string, PRCP: string, SNOW_AMOUNT: string, TMAX: string, TMIN: string, rain: int, snow: int, both: int, weather: string, count: bigint]>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joined_df.printSchema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "joined_df.coalesce(1).write.mode(\"overwrite\").csv(OUTPUT_PATH+\"daily_weather\", header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create daily trip data schema \n",
    "\n",
    "dailydata_schema = types.StructType([\n",
    "    types.StructField('DATE', types.TimestampType()),\n",
    "    types.StructField('PRCP', types.FloatType()),\n",
    "    types.StructField('SNOW_AMOUNT', types.FloatType()),\n",
    "    types.StructField('TMAX', types.IntegerType()),\n",
    "    types.StructField('TMIN', types.IntegerType()),\n",
    "    types.StructField('rain', types.IntegerType()),\n",
    "    types.StructField('snow', types.IntegerType()),\n",
    "    types.StructField('both', types.IntegerType()),\n",
    "    types.StructField('weather', types.StringType()),\n",
    "    types.StructField('count', types.IntegerType()),  \n",
    "])\n",
    "\n",
    "\n",
    "daily_data = spark.read.csv(OUTPUT_PATH+'daily_weather', header=True, schema=dailydata_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method DataFrame.printSchema of DataFrame[DATE: timestamp, PRCP: float, SNOW_AMOUNT: float, TMAX: int, TMIN: int, rain: int, snow: int, both: int, weather: string, count: int]>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "daily_data.printSchema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+----+-----------+----+----+----+----+----+-------+-----+\n",
      "|               DATE|PRCP|SNOW_AMOUNT|TMAX|TMIN|rain|snow|both|weather|count|\n",
      "+-------------------+----+-----------+----+----+----+----+----+-------+-----+\n",
      "|2010-09-20 00:00:00| 0.0|        0.0|  81|  64|   0|   0|   0|  sunny|  212|\n",
      "|2010-09-21 00:00:00| 0.0|        0.0|  80|  56|   0|   0|   0|  sunny|  324|\n",
      "|2010-09-22 00:00:00| 0.0|        0.0|  95|  67|   0|   0|   0|  sunny|  377|\n",
      "|2010-09-23 00:00:00| 0.0|        0.0|  93|  71|   0|   0|   0|  sunny|  373|\n",
      "|2010-09-24 00:00:00| 0.0|        0.0|  99|  72|   0|   0|   0|  sunny|  362|\n",
      "+-------------------+----+-----------+----+----+----+----+----+-------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "daily_data.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+----------+----------+\n",
      "|               DATE|dow_number|dow_string|\n",
      "+-------------------+----------+----------+\n",
      "|2010-09-20 00:00:00|         1|       Mon|\n",
      "|2010-09-21 00:00:00|         2|       Tue|\n",
      "|2010-09-22 00:00:00|         3|       Wed|\n",
      "|2010-09-23 00:00:00|         4|       Thu|\n",
      "|2010-09-24 00:00:00|         5|       Fri|\n",
      "+-------------------+----------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# create day of week columns \n",
    "\n",
    "from pyspark.sql.functions import date_format\n",
    "df_dof = daily_data.select('DATE', date_format('DATE', 'u').alias('dow_number'), date_format('DATE', 'E').alias('dow_string'))\n",
    "df_dof.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+----+-----------+----+----+----+----+----+-------+-----+----------+----------+\n",
      "|               DATE|PRCP|SNOW_AMOUNT|TMAX|TMIN|rain|snow|both|weather|count|dow_number|dow_string|\n",
      "+-------------------+----+-----------+----+----+----+----+----+-------+-----+----------+----------+\n",
      "|2010-09-20 00:00:00| 0.0|        0.0|  81|  64|   0|   0|   0|  sunny|  212|         1|       Mon|\n",
      "|2010-09-21 00:00:00| 0.0|        0.0|  80|  56|   0|   0|   0|  sunny|  324|         2|       Tue|\n",
      "|2010-09-22 00:00:00| 0.0|        0.0|  95|  67|   0|   0|   0|  sunny|  377|         3|       Wed|\n",
      "|2010-09-23 00:00:00| 0.0|        0.0|  93|  71|   0|   0|   0|  sunny|  373|         4|       Thu|\n",
      "|2010-09-24 00:00:00| 0.0|        0.0|  99|  72|   0|   0|   0|  sunny|  362|         5|       Fri|\n",
      "|2010-09-25 00:00:00| 0.0|        0.0|  93|  74|   0|   0|   0|  sunny|  539|         6|       Sat|\n",
      "|2010-09-26 00:00:00| 0.1|        0.0|  76|  63|   1|   0|   0|   rain|  537|         7|       Sun|\n",
      "|2010-09-27 00:00:00| 0.2|        0.0|  78|  64|   1|   0|   0|   rain|  286|         1|       Mon|\n",
      "|2010-09-28 00:00:00| 0.0|        0.0|  84|  65|   0|   0|   0|  sunny|  411|         2|       Tue|\n",
      "|2010-09-29 00:00:00|0.35|        0.0|  69|  61|   1|   0|   0|   rain|  422|         3|       Wed|\n",
      "|2010-09-30 00:00:00|4.66|        0.0|  81|  66|   1|   0|   0|   rain|  159|         4|       Thu|\n",
      "|2010-10-01 00:00:00|0.03|        0.0|  75|  60|   1|   0|   0|   rain|  578|         5|       Fri|\n",
      "|2010-10-02 00:00:00| 0.0|        0.0|  72|  52|   0|   0|   0|  sunny|  839|         6|       Sat|\n",
      "|2010-10-03 00:00:00| 0.4|        0.0|  67|  53|   1|   0|   0|   rain|  505|         7|       Sun|\n",
      "|2010-10-04 00:00:00|0.38|        0.0|  57|  50|   1|   0|   0|   rain|  326|         1|       Mon|\n",
      "|2010-10-05 00:00:00| 0.0|        0.0|  63|  50|   0|   0|   0|  sunny|  624|         2|       Tue|\n",
      "|2010-10-06 00:00:00|0.03|        0.0|  63|  52|   1|   0|   0|   rain|  663|         3|       Wed|\n",
      "|2010-10-07 00:00:00| 0.0|        0.0|  77|  51|   0|   0|   0|  sunny|  869|         4|       Thu|\n",
      "|2010-10-08 00:00:00| 0.0|        0.0|  79|  53|   0|   0|   0|  sunny|  914|         5|       Fri|\n",
      "|2010-10-09 00:00:00| 0.0|        0.0|  83|  54|   0|   0|   0|  sunny| 1109|         6|       Sat|\n",
      "+-------------------+----+-----------+----+----+----+----+----+-------+-----+----------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "daily_data = daily_data.join(df_dof, 'DATE')\n",
    "# daily_data = daily_data.withColumn(dow_number, daily_data[\"dow_number\"].cast(IntegerType()))\n",
    "daily_data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method DataFrame.printSchema of DataFrame[DATE: timestamp, PRCP: float, SNOW_AMOUNT: float, TMAX: int, TMIN: int, rain: int, snow: int, both: int, weather: string, count: int, dow_number: string, dow_string: string]>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "daily_data.printSchema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Learning with process data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- DATE: timestamp (nullable = true)\n",
      " |-- PRCP: float (nullable = true)\n",
      " |-- SNOW_AMOUNT: float (nullable = true)\n",
      " |-- TMAX: integer (nullable = true)\n",
      " |-- TMIN: integer (nullable = true)\n",
      " |-- weather: string (nullable = true)\n",
      " |-- dow_number: string (nullable = true)\n",
      " |-- dow_string: string (nullable = true)\n",
      " |-- count: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# select useful columns \n",
    "\n",
    "ml = daily_data.select(\"DATE\", \n",
    "              \"PRCP\", \n",
    "              \"SNOW_AMOUNT\", \n",
    "              \"TMAX\", \n",
    "              \"TMIN\", \n",
    "              \"weather\", \n",
    "              \"dow_number\", \n",
    "              \"dow_string\",\n",
    "              \"count\")\n",
    "ml.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+----+-----------+----+----+-------+----------+----------+-----+\n",
      "|               DATE|PRCP|SNOW_AMOUNT|TMAX|TMIN|weather|dow_number|dow_string|count|\n",
      "+-------------------+----+-----------+----+----+-------+----------+----------+-----+\n",
      "|2010-09-20 00:00:00| 0.0|        0.0|  81|  64|  sunny|         1|       Mon|  212|\n",
      "|2010-09-21 00:00:00| 0.0|        0.0|  80|  56|  sunny|         2|       Tue|  324|\n",
      "|2010-09-22 00:00:00| 0.0|        0.0|  95|  67|  sunny|         3|       Wed|  377|\n",
      "|2010-09-23 00:00:00| 0.0|        0.0|  93|  71|  sunny|         4|       Thu|  373|\n",
      "|2010-09-24 00:00:00| 0.0|        0.0|  99|  72|  sunny|         5|       Fri|  362|\n",
      "|2010-09-25 00:00:00| 0.0|        0.0|  93|  74|  sunny|         6|       Sat|  539|\n",
      "|2010-09-26 00:00:00| 0.1|        0.0|  76|  63|   rain|         7|       Sun|  537|\n",
      "|2010-09-27 00:00:00| 0.2|        0.0|  78|  64|   rain|         1|       Mon|  286|\n",
      "|2010-09-28 00:00:00| 0.0|        0.0|  84|  65|  sunny|         2|       Tue|  411|\n",
      "|2010-09-29 00:00:00|0.35|        0.0|  69|  61|   rain|         3|       Wed|  422|\n",
      "+-------------------+----+-----------+----+----+-------+----------+----------+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ml.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression with only numerical data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-----+\n",
      "|           features|count|\n",
      "+-------------------+-----+\n",
      "|[0.0,0.0,81.0,64.0]|  212|\n",
      "|[0.0,0.0,80.0,56.0]|  324|\n",
      "|[0.0,0.0,95.0,67.0]|  377|\n",
      "+-------------------+-----+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "vectorAssembler = VectorAssembler(inputCols = ['PRCP', 'SNOW_AMOUNT', 'TMAX', 'TMIN'], outputCol = 'features')\n",
    "vector_df = vectorAssembler.transform(ml)\n",
    "vector_df = vector_df.select(['features', 'count'])\n",
    "vector_df.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create train and test data\n",
    "train_data, test_data = vector_df.randomSplit([.8,.2],seed=1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficients: [-2312.2633608085325,-116.81584741125842,124.53870323717341,9.081777957576016]\n",
      "Intercept: -1001.8063933212435\n"
     ]
    }
   ],
   "source": [
    "# import `LinearRegression`\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "\n",
    "# Initialize `lr`\n",
    "lr = LinearRegression(featuresCol = 'features', labelCol='count', maxIter=10, regParam=0.3, elasticNetParam=0.8)\n",
    "lr_model = lr.fit(train_data)\n",
    "print(\"Coefficients: \" + str(lr_model.coefficients))\n",
    "print(\"Intercept: \" + str(lr_model.intercept))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 2908.028212\n",
      "r2: 0.435713\n"
     ]
    }
   ],
   "source": [
    "# train model and see training error\n",
    "trainingSummary = lr_model.summary\n",
    "print(\"RMSE: %f\" % trainingSummary.rootMeanSquaredError)\n",
    "print(\"r2: %f\" % trainingSummary.r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------+\n",
      "|summary|            count|\n",
      "+-------+-----------------+\n",
      "|  count|             2684|\n",
      "|   mean|7700.555141579732|\n",
      "| stddev|3871.947212850098|\n",
      "|    min|               21|\n",
      "|    max|            18346|\n",
      "+-------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# train data summary\n",
    "train_data.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+-----+-------------------+\n",
      "|        prediction|count|           features|\n",
      "+------------------+-----+-------------------+\n",
      "|1446.1649698032356| 1426| [0.0,0.0,19.0,9.0]|\n",
      "|1667.9970424048543| 2282| [0.0,0.0,21.0,6.0]|\n",
      "|1713.4059321927343| 1489|[0.0,0.0,21.0,11.0]|\n",
      "| 2345.181226336177| 2952|[0.0,0.0,26.0,12.0]|\n",
      "| 2870.581373157599| 3035|[0.0,0.0,30.0,15.0]|\n",
      "+------------------+-----+-------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "R Squared (R2) on test data = 0.432235\n"
     ]
    }
   ],
   "source": [
    "# predict on test data\n",
    "lr_predictions = lr_model.transform(test_data)\n",
    "lr_predictions.select(\"prediction\",\"count\",\"features\").show(5)\n",
    "\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "lr_evaluator = RegressionEvaluator(predictionCol=\"prediction\", \\\n",
    "                 labelCol=\"count\",metricName=\"r2\")\n",
    "print(\"R Squared (R2) on test data = %g\" % lr_evaluator.evaluate(lr_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression with categorical data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+----+-----------+----+----+-------+----------+----------+-----+\n",
      "|               DATE|PRCP|SNOW_AMOUNT|TMAX|TMIN|weather|dow_number|dow_string|count|\n",
      "+-------------------+----+-----------+----+----+-------+----------+----------+-----+\n",
      "|2010-09-20 00:00:00| 0.0|        0.0|  81|  64|  sunny|         1|       Mon|  212|\n",
      "|2010-09-21 00:00:00| 0.0|        0.0|  80|  56|  sunny|         2|       Tue|  324|\n",
      "|2010-09-22 00:00:00| 0.0|        0.0|  95|  67|  sunny|         3|       Wed|  377|\n",
      "+-------------------+----+-----------+----+----+-------+----------+----------+-----+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ml.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, test_df = ml.randomSplit([.8,.2],seed=1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode catogorical data with OneHotEncoder\n",
    "from pyspark.ml.feature import StringIndexer, VectorAssembler, SQLTransformer, OneHotEncoderEstimator\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "\n",
    "categorical_variables = ['weather', 'dow_string']\n",
    "indexers = [StringIndexer(inputCol=column, outputCol=column+\"-index\") for column in categorical_variables]\n",
    "\n",
    "encoder = OneHotEncoderEstimator(\n",
    "    inputCols=[indexer.getOutputCol() for indexer in indexers],\n",
    "    outputCols=[\"{0}-encoded\".format(indexer.getOutputCol()) for indexer in indexers]\n",
    ")\n",
    "\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=encoder.getOutputCols(),\n",
    "    outputCol=\"categorical-features\"\n",
    ")\n",
    "\n",
    "pipeline = Pipeline(stages=indexers + [encoder, assembler])\n",
    "\n",
    "train_df = pipeline.fit(train_df).transform(train_df)\n",
    "test_df = pipeline.fit(test_df).transform(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select out continuous_variables \n",
    "continuous_variables = ['PRCP', 'SNOW_AMOUNT', 'TMAX', 'TMIN']\n",
    "\n",
    "# combine categorical-features with continuous-features\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=['categorical-features', *continuous_variables],\n",
    "    outputCol='features'\n",
    ")\n",
    "\n",
    "# transform train and test data with assembler\n",
    "train_df = assembler.transform(train_df)\n",
    "test_df = assembler.transform(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficients: [731.6846174072824,-531.936286065667,-548.6379838607589,197.55078332668162,670.5148664220861,768.1255661088442,667.0658613703243,697.667226200207,462.7270717130937,-1667.634547035611,45.08461271246068,97.39768837537152,41.14999281149254]\n",
      "Intercept: -1753.4372440733168\n"
     ]
    }
   ],
   "source": [
    "# initialize `lr`\n",
    "lr = LinearRegression(featuresCol = 'features', labelCol='count', maxIter=10, regParam=0.3, elasticNetParam=0.8)\n",
    "lr_model = lr.fit(train_df)\n",
    "print(\"Coefficients: \" + str(lr_model.coefficients))\n",
    "print(\"Intercept: \" + str(lr_model.intercept))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 2871.962186\n",
      "r2: 0.450877\n"
     ]
    }
   ],
   "source": [
    "# train data statistics\n",
    "trainingSummary = lr_model.summary\n",
    "print(\"RMSE: %f\" % trainingSummary.rootMeanSquaredError)\n",
    "print(\"r2: %f\" % trainingSummary.r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+-----+--------------------+\n",
      "|        prediction|count|            features|\n",
      "+------------------+-----+--------------------+\n",
      "| 9501.059671674582|  212|(13,[0,11,12],[1....|\n",
      "| 9741.527902177593|  324|(13,[0,6,11,12],[...|\n",
      "|11658.592153786347|  377|(13,[0,4,11,12],[...|\n",
      "| 5951.718164592574|  505|(13,[1,3,9,11,12]...|\n",
      "|   8426.9153895427| 1374|(13,[0,5,11,12],[...|\n",
      "+------------------+-----+--------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "R Squared (R2) on test data = 0.461891\n"
     ]
    }
   ],
   "source": [
    "# test data statistics\n",
    "lr_predictions = lr_model.transform(test_df)\n",
    "lr_predictions.select(\"prediction\",\"count\",\"features\").show(5)\n",
    "\n",
    "lr_evaluator = RegressionEvaluator(predictionCol=\"prediction\", \\\n",
    "                 labelCol=\"count\",metricName=\"r2\")\n",
    "print(\"R Squared (R2) on test data = %g\" % lr_evaluator.evaluate(lr_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.regression import DecisionTreeRegressor\n",
    "from pyspark.ml.feature import VectorIndexer\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+----+-----------+----+----+-------+----------+----------+-----+\n",
      "|               DATE|PRCP|SNOW_AMOUNT|TMAX|TMIN|weather|dow_number|dow_string|count|\n",
      "+-------------------+----+-----------+----+----+-------+----------+----------+-----+\n",
      "|2010-09-20 00:00:00| 0.0|        0.0|  81|  64|  sunny|         1|       Mon|  212|\n",
      "|2010-09-21 00:00:00| 0.0|        0.0|  80|  56|  sunny|         2|       Tue|  324|\n",
      "|2010-09-22 00:00:00| 0.0|        0.0|  95|  67|  sunny|         3|       Wed|  377|\n",
      "|2010-09-23 00:00:00| 0.0|        0.0|  93|  71|  sunny|         4|       Thu|  373|\n",
      "|2010-09-24 00:00:00| 0.0|        0.0|  99|  72|  sunny|         5|       Fri|  362|\n",
      "+-------------------+----+-----------+----+----+-------+----------+----------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ml.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+\n",
      "|label|            features|\n",
      "+-----+--------------------+\n",
      "|  0.0|(692,[98,99,100,1...|\n",
      "|  0.0|(692,[100,101,102...|\n",
      "|  0.0|(692,[122,123,148...|\n",
      "+-----+--------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Automatically identify categorical features, and index them.\n",
    "# We specify maxCategories so features with > 4 distinct values are treated as continuous.\n",
    "featureIndexer =\\\n",
    "    VectorIndexer(inputCol=\"features\", outputCol=\"indexedFeatures\", maxCategories=4).fit(data)\n",
    "\n",
    "# Split the data into training and test sets (30% held out for testing)\n",
    "(trainingData, testData) = data.randomSplit([0.7, 0.3])\n",
    "trainingData.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+--------------------+----------+\n",
      "|label|            features|     indexedFeatures|prediction|\n",
      "+-----+--------------------+--------------------+----------+\n",
      "|  0.0|(692,[95,96,97,12...|(692,[95,96,97,12...|       0.0|\n",
      "|  0.0|(692,[121,122,123...|(692,[121,122,123...|       0.0|\n",
      "|  0.0|(692,[122,123,124...|(692,[122,123,124...|       0.0|\n",
      "|  0.0|(692,[123,124,125...|(692,[123,124,125...|       0.0|\n",
      "|  0.0|(692,[124,125,126...|(692,[124,125,126...|       0.0|\n",
      "+-----+--------------------+--------------------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Train a DecisionTree model.\n",
    "dt = DecisionTreeRegressor(featuresCol=\"indexedFeatures\")\n",
    "# Chain indexer and tree in a Pipeline\n",
    "pipeline = Pipeline(stages=[featureIndexer, dt])\n",
    "\n",
    "# Train model.  This also runs the indexer.\n",
    "model = pipeline.fit(trainingData)\n",
    "\n",
    "# Make predictions.\n",
    "predictions = model.transform(testData)\n",
    "\n",
    "predictions.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+--------------------+\n",
      "|prediction|label|            features|\n",
      "+----------+-----+--------------------+\n",
      "|       0.0|  0.0|(692,[95,96,97,12...|\n",
      "|       0.0|  0.0|(692,[121,122,123...|\n",
      "|       0.0|  0.0|(692,[122,123,124...|\n",
      "|       0.0|  0.0|(692,[123,124,125...|\n",
      "|       0.0|  0.0|(692,[124,125,126...|\n",
      "+----------+-----+--------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "Root Mean Squared Error (RMSE) on test data = 0.164399\n",
      "DecisionTreeRegressionModel (uid=DecisionTreeRegressor_ad3228f029a6) of depth 1 with 3 nodes\n"
     ]
    }
   ],
   "source": [
    "# Select example rows to display.\n",
    "predictions.select(\"prediction\", \"label\", \"features\").show(5)\n",
    "\n",
    "# Select (prediction, true label) and compute test error\n",
    "evaluator = RegressionEvaluator(\n",
    "    labelCol=\"label\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "rmse = evaluator.evaluate(predictions)\n",
    "print(\"Root Mean Squared Error (RMSE) on test data = %g\" % rmse)\n",
    "\n",
    "treeModel = model.stages[1]\n",
    "# summary only\n",
    "print(treeModel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.regression import DecisionTreeRegressor\n",
    "from pyspark.ml.feature import VectorIndexer\n",
    "from pyspark.ml.evaluation import RegressionEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- PRCP: float (nullable = true)\n",
      " |-- SNOW_AMOUNT: float (nullable = true)\n",
      " |-- TMAX: integer (nullable = true)\n",
      " |-- TMIN: integer (nullable = true)\n",
      " |-- weather: string (nullable = true)\n",
      " |-- dow_string: string (nullable = true)\n",
      " |-- count: integer (nullable = true)\n",
      "\n",
      "+----+-----------+----+----+-------+----------+-----+\n",
      "|PRCP|SNOW_AMOUNT|TMAX|TMIN|weather|dow_string|count|\n",
      "+----+-----------+----+----+-------+----------+-----+\n",
      "| 0.0|        0.0|  81|  64|  sunny|       Mon|  212|\n",
      "| 0.0|        0.0|  80|  56|  sunny|       Tue|  324|\n",
      "| 0.0|        0.0|  95|  67|  sunny|       Wed|  377|\n",
      "+----+-----------+----+----+-------+----------+-----+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# select useful columns \n",
    "ml = ml.select(\"PRCP\", \n",
    "              \"SNOW_AMOUNT\", \n",
    "              \"TMAX\", \n",
    "              \"TMIN\", \n",
    "              \"weather\", \n",
    "              \"dow_string\",\n",
    "              \"count\")\n",
    "ml.printSchema()\n",
    "ml.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----------+----+----+-------+----------+-----+-------------+----------------+---------------------+------------------------+--------------------+\n",
      "|PRCP|SNOW_AMOUNT|TMAX|TMIN|weather|dow_string|count|weather-index|dow_string-index|weather-index-encoded|dow_string-index-encoded|categorical-features|\n",
      "+----+-----------+----+----+-------+----------+-----+-------------+----------------+---------------------+------------------------+--------------------+\n",
      "| 0.0|        0.0|  81|  64|  sunny|       Mon|  212|          0.0|             3.0|        (3,[0],[1.0])|           (6,[3],[1.0])| (9,[0,6],[1.0,1.0])|\n",
      "| 0.0|        0.0|  80|  56|  sunny|       Tue|  324|          0.0|             2.0|        (3,[0],[1.0])|           (6,[2],[1.0])| (9,[0,5],[1.0,1.0])|\n",
      "| 0.0|        0.0|  95|  67|  sunny|       Wed|  377|          0.0|             1.0|        (3,[0],[1.0])|           (6,[1],[1.0])| (9,[0,4],[1.0,1.0])|\n",
      "+----+-----------+----+----+-------+----------+-----+-------------+----------------+---------------------+------------------------+--------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# encode catogorical data with OneHotEncoder\n",
    "from pyspark.ml.feature import StringIndexer, VectorAssembler, SQLTransformer, OneHotEncoderEstimator\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "\n",
    "categorical_variables = ['weather', 'dow_string']\n",
    "indexers = [StringIndexer(inputCol=column, outputCol=column+\"-index\") for column in categorical_variables]\n",
    "\n",
    "encoder = OneHotEncoderEstimator(\n",
    "    inputCols=[indexer.getOutputCol() for indexer in indexers],\n",
    "    outputCols=[\"{0}-encoded\".format(indexer.getOutputCol()) for indexer in indexers]\n",
    ")\n",
    "\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=encoder.getOutputCols(),\n",
    "    outputCol=\"categorical-features\"\n",
    ")\n",
    "\n",
    "pipeline = Pipeline(stages=indexers + [encoder, assembler])\n",
    "\n",
    "ml = pipeline.fit(ml).transform(ml)\n",
    "ml.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----------+----+----+-------+----------+-----+-------------+----------------+---------------------+------------------------+--------------------+--------------------+\n",
      "|PRCP|SNOW_AMOUNT|TMAX|TMIN|weather|dow_string|count|weather-index|dow_string-index|weather-index-encoded|dow_string-index-encoded|categorical-features|            features|\n",
      "+----+-----------+----+----+-------+----------+-----+-------------+----------------+---------------------+------------------------+--------------------+--------------------+\n",
      "| 0.0|        0.0|  81|  64|  sunny|       Mon|  212|          0.0|             3.0|        (3,[0],[1.0])|           (6,[3],[1.0])| (9,[0,6],[1.0,1.0])|(13,[0,6,11,12],[...|\n",
      "| 0.0|        0.0|  80|  56|  sunny|       Tue|  324|          0.0|             2.0|        (3,[0],[1.0])|           (6,[2],[1.0])| (9,[0,5],[1.0,1.0])|(13,[0,5,11,12],[...|\n",
      "| 0.0|        0.0|  95|  67|  sunny|       Wed|  377|          0.0|             1.0|        (3,[0],[1.0])|           (6,[1],[1.0])| (9,[0,4],[1.0,1.0])|(13,[0,4,11,12],[...|\n",
      "+----+-----------+----+----+-------+----------+-----+-------------+----------------+---------------------+------------------------+--------------------+--------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# select out continuous_variables \n",
    "continuous_variables = ['PRCP', 'SNOW_AMOUNT', 'TMAX', 'TMIN']\n",
    "\n",
    "# combine categorical-features with continuous-features\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=['categorical-features',*continuous_variables],\n",
    "    outputCol='features'\n",
    ")\n",
    "\n",
    "# transform train and test data with assembler\n",
    "ml = assembler.transform(ml)\n",
    "ml.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------+\n",
      "|features                            |\n",
      "+------------------------------------+\n",
      "|(13,[0,6,11,12],[1.0,1.0,81.0,64.0])|\n",
      "|(13,[0,5,11,12],[1.0,1.0,80.0,56.0])|\n",
      "|(13,[0,4,11,12],[1.0,1.0,95.0,67.0])|\n",
      "|(13,[0,3,11,12],[1.0,1.0,93.0,71.0])|\n",
      "|(13,[0,7,11,12],[1.0,1.0,99.0,72.0])|\n",
      "+------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ml.select(\"features\").show(5, False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, test_df = ml.randomSplit([.8,.2],seed=1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+-----+\n",
      "|        prediction|count|\n",
      "+------------------+-----+\n",
      "|2972.8137931034485| 1426|\n",
      "|2972.8137931034485| 2282|\n",
      "|2972.8137931034485| 1489|\n",
      "|2972.8137931034485| 2952|\n",
      "|2972.8137931034485| 4010|\n",
      "+------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Train a DecisionTree model.\n",
    "dt = DecisionTreeRegressor(featuresCol=\"features\",labelCol='count')\n",
    "\n",
    "# Train model.  This also runs the indexer.dt\n",
    "dt_model = dt.fit(train_df)\n",
    "\n",
    "# Make predictions.\n",
    "predictions = dt_model.transform(test_df)\n",
    "\n",
    "# Select example rows to display.\n",
    "predictions.select(\"prediction\", \"count\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Mean Squared Error (RMSE) on test data = 2834.34\n",
      "DecisionTreeRegressionModel (uid=DecisionTreeRegressor_4c2f97ec28ff) of depth 5 with 63 nodes\n"
     ]
    }
   ],
   "source": [
    "# Select (prediction, true label) and compute test error\n",
    "evaluator = RegressionEvaluator(\n",
    "    labelCol=\"count\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "rmse = evaluator.evaluate(predictions)\n",
    "print(\"Root Mean Squared Error (RMSE) on test data = %g\" % rmse)\n",
    "\n",
    "# summary only\n",
    "print(dt_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R Squared (R2) on test data = 0.452394\n"
     ]
    }
   ],
   "source": [
    "evaluator = RegressionEvaluator(predictionCol=\"prediction\", \\\n",
    "                 labelCol=\"count\",metricName=\"r2\")\n",
    "print(\"R Squared (R2) on test data = %g\" % evaluator.evaluate(predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random forest regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.regression import RandomForestRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+-----+\n",
      "|        prediction|count|\n",
      "+------------------+-----+\n",
      "| 4279.499625231992| 1426|\n",
      "| 3750.725116437977| 2282|\n",
      "| 3723.934951932032| 1489|\n",
      "| 3837.527992554989| 2952|\n",
      "|3822.8632461716797| 4010|\n",
      "+------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Train a DecisionTree model.\n",
    "rf = RandomForestRegressor(featuresCol=\"features\",labelCol='count')\n",
    "\n",
    "# Train model.  This also runs the indexer.dt\n",
    "rf_model = rf.fit(train_df)\n",
    "\n",
    "# Make predictions.\n",
    "predictions = rf_model.transform(test_df)\n",
    "\n",
    "# Select example rows to display.\n",
    "predictions.select(\"prediction\", \"count\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Mean Squared Error (RMSE) on test data = 2779.86\n",
      "RandomForestRegressionModel (uid=RandomForestRegressor_bdce62fab1be) with 20 trees\n"
     ]
    }
   ],
   "source": [
    "# Select (prediction, true label) and compute test error\n",
    "evaluator = RegressionEvaluator(\n",
    "    labelCol=\"count\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "rmse = evaluator.evaluate(predictions)\n",
    "print(\"Root Mean Squared Error (RMSE) on test data = %g\" % rmse)\n",
    "\n",
    "# summary only\n",
    "print(rf_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R Squared (R2) on test data = 0.473244\n"
     ]
    }
   ],
   "source": [
    "evaluator = RegressionEvaluator(predictionCol=\"prediction\", \\\n",
    "                 labelCol=\"count\",metricName=\"r2\")\n",
    "print(\"R Squared (R2) on test data = %g\" % evaluator.evaluate(predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient-boosted tree regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import GBTRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+-----+\n",
      "|        prediction|count|\n",
      "+------------------+-----+\n",
      "| 3572.324396937591| 1426|\n",
      "| 2956.649978628918| 2282|\n",
      "|2554.4041454393387| 1489|\n",
      "|2949.8051331274987| 2952|\n",
      "| 3098.102998961212| 4010|\n",
      "+------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Train a DecisionTree model.\n",
    "gbt = GBTRegressor(featuresCol=\"features\",labelCol='count')\n",
    "\n",
    "# Train model.  This also runs the indexer.dt\n",
    "gbt_model = gbt.fit(train_df)\n",
    "\n",
    "# Make predictions.\n",
    "predictions = gbt_model.transform(test_df)\n",
    "\n",
    "# Select example rows to display.\n",
    "predictions.select(\"prediction\", \"count\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Mean Squared Error (RMSE) on test data = 2892.59\n",
      "GBTRegressionModel (uid=GBTRegressor_3e57796708b8) with 20 trees\n"
     ]
    }
   ],
   "source": [
    "# Select (prediction, true label) and compute test error\n",
    "evaluator = RegressionEvaluator(\n",
    "    labelCol=\"count\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "rmse = evaluator.evaluate(predictions)\n",
    "print(\"Root Mean Squared Error (RMSE) on test data = %g\" % rmse)\n",
    "\n",
    "# summary only\n",
    "print(gbt_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R Squared (R2) on test data = 0.429655\n"
     ]
    }
   ],
   "source": [
    "evaluator = RegressionEvaluator(predictionCol=\"prediction\", \\\n",
    "                 labelCol=\"count\",metricName=\"r2\")\n",
    "print(\"R Squared (R2) on test data = %g\" % evaluator.evaluate(predictions))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
